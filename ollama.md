# üß† Ollama

<p align="center">
  <img src="/img/openai.png" alt="openai" width="800">
</p>

### √çndice

1. [¬øQu√© es Ollama?](#-qu√©-es-ollama)
2. [Instalaci√≥n de Ollama](#Ô∏è-instalaci√≥n-de-ollama)
3. [Integraci√≥n de Ollama con n8n](#-integraci√≥n-de-ollama-con-n8n)
4. [Enlaces de referencia](#-enlaces-de-referencia)

---

### üß† ¬øQu√© es Ollama?

**Ollama** es una herramienta que permite ejecutar **modelos de lenguaje grandes (LLMs)** localmente, sin depender de servicios externos en la nube. Utiliza modelos como **Mistral**, **LLaMA**, **Gemma** o **Phi**, permitiendo generar respuestas de texto de forma r√°pida, privada y totalmente controlada.

En el contexto de este proyecto, Ollama se utiliza para **responder autom√°ticamente a los usuarios de Telegram** con informaci√≥n √∫ltil o instrucciones, simulando un asistente de inteligencia artificial.

### Ventajas de usar Ollama:

* ‚úÖ No requiere conexi√≥n con APIs externas (como OpenAI).
* ‚úÖ Ejecuta modelos en local o en servidor propio.
* ‚úÖ Compatible con m√∫ltiples modelos.
* ‚úÖ Ideal para entornos offline o privados.

---

## ‚öôÔ∏è Instalaci√≥n de Ollama

La instalaci√≥n de Ollama se realiz√≥ en un servidor independiente llamado `ollama`. Los pasos seguidos fueron:

1. **Descargar e instalar Ollama**:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

2. **Ejecutar el modelo Mistral**:

```bash
ollama run mistral
```

Esto descarga y activa el modelo **Mistral**, que se usar√° para generar respuestas autom√°ticas.

3. **Configurar acceso remoto**:

Se configur√≥ el servicio para escuchar en todas las interfaces de red, permitiendo que `n8n` acceda al puerto 11434:

```bash
nano ~/.ollama/config.toml
```

Con el siguiente contenido:

```toml
[server]
address = "0.0.0.0:11434"
```

Luego se inici√≥ el servicio:

```bash
ollama serve
```

4. **Abrir el puerto 11434 en el firewall**:

```bash
ufw allow 11434
```

---

## üîó Integraci√≥n de Ollama con n8n

La comunicaci√≥n entre `n8n` y `ollama` se realiza directamente mediante la configuraci√≥n de credenciales HTTP internas en el propio panel de n8n.

### Paso 1: Crear credencial Ollama en n8n

Desde el panel de n8n se ha creado una credencial de tipo **Ollama** especificando la IP del servidor Ollama (por ejemplo: `http://172.29.83.185:11434`). Esta configuraci√≥n ha sido probada correctamente.


<p align="center">
  <img src="/img/cuentaOllama.png" alt="Flujo Ollama modelo Mistral" />
</p>

### Paso 2: Crear flujo en n8n con el modelo Mistral

Dentro del flujo de trabajo:

1. **Telegram Trigger**: captura el mensaje del usuario.
2. **Set Node**: recoge el contenido del mensaje (`{{$json["message"]["text"]}}`).
3. **Ollama Node**:

   * Credencial: `Ollama account`
   * Modelo: `mistral:latest`
     
<p align="center">
  <img src="/img/credencial.png" alt="Credencial Ollama en n8n" />
</p>

4. **Telegram Send Message**: devuelve la respuesta generada al usuario.

Este m√©todo simplifica la integraci√≥n, sin necesidad de usar nodos HTTP manuales o configuraciones adicionales. Todo queda encapsulado mediante el nodo oficial de Ollama en n8n.

---

## üìå Enlaces de referencia

* Sitio oficial de Ollama: [https://ollama.com](https://ollama.com)
* Documentaci√≥n API: [https://github.com/jmorganca/ollama/blob/main/docs/api.md](https://github.com/jmorganca/ollama/blob/main/docs/api.md)
* ZeroTier: [https://www.zerotier.com](https://www.zerotier.com)
* n8n: [https://n8n.io](https://n8n.io)
