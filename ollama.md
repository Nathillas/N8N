Claro, aqu铆 tienes el **documento completo en Markdown** listo para copiar, con el 铆ndice corto enlazado y sin los apartados de *Ejemplo de uso con API* y *Ventajas de la integraci贸n*:

---

````markdown
#  Ollama en el Proyecto

##  ndice de Contenidos - Ollama

- [驴Qu茅 es Ollama?](#qu茅-es-ollama)
- [Instalaci贸n en AWS EC2](#instalaci贸n-de-ollama-en-una-instancia-aws-ec2)
- [Integraci贸n en el proyecto](#integraci贸n-de-ollama-en-el-proyecto)
- [Documentaci贸n oficial](#-enlace-a-documentaci贸n-oficial)

---

## 驴Qu茅 es Ollama?

[Ollama](https://ollama.com/) es una herramienta dise帽ada para ejecutar modelos de lenguaje grande (LLMs) de manera local y sencilla. Permite ejecutar modelos como LLaMA, Mistral, Gemma, entre otros, a trav茅s de una interfaz de l铆nea de comandos y una API REST. Ollama es ideal para desarrolladores que buscan privacidad, bajo coste y latencia m铆nima, sin depender de servicios en la nube de terceros.

---

## Instalaci贸n de Ollama en una instancia AWS EC2

### Requisitos Previos

- Instancia EC2 con Ubuntu 20.04 o 22.04 (x86_64).
- Clave `.pem` para conexi贸n SSH.
- Acceso al puerto `11434` si se desea exponer el servicio.
- Usuario `ubuntu` por defecto en EC2.

### 1. Conexi贸n a la instancia EC2

```bash
ssh -i /ruta/a/tu/llave.pem ubuntu@IP_DE_TU_INSTANCIA
````

### 2. Actualizar el sistema

```bash
sudo apt update && sudo apt upgrade -y
```

### 3. Instalar herramientas b谩sicas

```bash
sudo apt install curl -y
```

### 4. Instalar Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 5. Iniciar el servicio de Ollama

```bash
ollama serve
```

Para ejecutarlo en segundo plano:

```bash
nohup ollama serve > /dev/null 2>&1 &
```

### 6. Verificar funcionamiento

```bash
curl http://localhost:11434
```

Debe responder con algo como:

```json
{"models":[]}
```

### 7. Acceso externo (opcional)

Aseg煤rate de abrir el puerto `11434` en el grupo de seguridad de EC2. Luego puedes acceder a la API desde fuera:

```bash
curl http://TU_IP_PUBLICA:11434
```

---

## Integraci贸n de Ollama en el Proyecto

Ollama se ha integrado en este proyecto como motor local para ejecutar modelos de lenguaje natural, eliminando la necesidad de depender de APIs externas como OpenAI o Hugging Face.

### Arquitectura de la integraci贸n

* **Infraestructura:** Ollama se ejecuta en una instancia EC2 dedicada con Ubuntu.
* **API REST:** Las aplicaciones del proyecto (como n8n o scripts personalizados) se comunican con Ollama mediante peticiones HTTP a `http://IP_SERVIDOR:11434`.
* **Modelo utilizado:** Se ha desplegado el modelo `mistral` con:

  ```bash
  ollama run mistral
  ```

### Casos de uso integrados

* Generaci贸n autom谩tica de respuestas.
* Resumen de textos y correos.
* Automatizaci贸n con [n8n](https://n8n.io/).
* Clasificaci贸n o an谩lisis de entradas de formularios.

---

##  Enlace a documentaci贸n oficial

 [https://ollama.com](https://ollama.com)

```


